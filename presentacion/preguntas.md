cosas en el aire:

porque al pasar de dimension n a dimension 3 saltas de una Yn a Y_1,Y_2.

Las parciales solo se estan haciendo para el tipo 1. Tente preparada el salto de una a otra.

Lo de los puntos criticos esta un poco en el aire. Ten en mente las cuentas que se hacen.

Te van a decir que porque n=20, y les vas a poner otro n para darles en la boca. Si te apuras de tiempo jugar con el sleep y que la chupen.

Integracion-> que es f(polinomio esférico)(no tiene sentido otra cosa)
Triangulacion -> Existen varias formas de hacerlas la mas tipica es la de delauny.
Triangulacion-> tener en la manga los resultados para sacar la I(f)(tirar de referencia)
Triangulacion->c_f ->This assumes only that the function f is Lipschitz continuous over S 2 with a
Lipschitz constant c_	f .
Es un poco arriesgado decir es conocido que..
The data values f i will often contain experimental error.


---------------

¿En que has programado? ----> Piton
¿Porque agrupar?¿Porque agrupar esas?¿Porque channel?Lo de agrupar se va a quedar un poco en el aire.
¿Ruido?¿Outliers?
Guardarme en el bolsillo lo de feature_importance.
Guardarme gradient boosting.
¿Te van a preguntar porque CUSBoost no te tira?¿Te van a decir que vaya mierda de alternativas que no funcionana?¿Si dices que RUSBoost es prometedor te van a a tirar por ahi?
¿Porque boosting y no otro?
¿que parametros has optimizado?
¿metodo de los centroides?
¿quiza alguien te diga que que es undersampling?
Guardate algo de deep learning y se la cuelas a Fernandito.
Realmente si tu conjunto de datos daba 0.86 y Rus da 0.87 has mejorado el boosting del que partias.


-------
¿Vias futuras?
